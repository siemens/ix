"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[40265],{35245:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});var s=o(97458),i=o(16436);const t={sidebar_position:8,title:"Ad hoc conversations"},a="Ad hoc conversations",r={id:"conversational-design/designing-conversations/ad-hoc-conversations",title:"Ad hoc conversations",description:"Overview",source:"@site/docs/conversational-design/designing-conversations/ad-hoc-conversations.md",sourceDirName:"conversational-design/designing-conversations",slug:"/conversational-design/designing-conversations/ad-hoc-conversations",permalink:"/docs/conversational-design/designing-conversations/ad-hoc-conversations",draft:!1,unlisted:!1,editUrl:"https://www.github.com/siemens/ix/edit/main/packages/documentation/docs/conversational-design/designing-conversations/ad-hoc-conversations.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_position:8,title:"Ad hoc conversations"},sidebar:"mySidebar",previous:{title:"Troubleshooting",permalink:"/docs/conversational-design/designing-conversations/troubleshooting"},next:{title:"Ending conversations",permalink:"/docs/conversational-design/designing-conversations/ending-conversations"}},c={},d=[{value:"Overview",id:"overview",level:2},{value:"Examples",id:"examples",level:2},{value:"Dos and Don\u2019ts",id:"dos-and-donts",level:2}];function l(e){const n={h1:"h1",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"ad-hoc-conversations",children:"Ad hoc conversations"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Even in industrial scenarios, users test and play with chatbots. Some ad hoc interactions can be fun and build relationships between your users and chatbots, however some can be undesirable and damaging for your brand. To mitigate this risk, it\u2019s important to implement filters and set up clear guidelines. Companies and projects can decide if they need to implement more than a basic response and give, for example, a warning message. Either way, most chatbots respond in the same way when it comes to offensive or inappropriate user queries."}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"/figma/../img/figma_error.png",alt:"Apology example"})}),"\n",(0,s.jsx)(n.p,{children:"Universally, chatbots start with an apology before anything else, even if it was the user that added inappropriate or offensive input."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"/figma/../img/figma_error.png",alt:"Extended apology example"})}),"\n",(0,s.jsx)(n.p,{children:"Here the chatbot seeks to move the user back on course and reminds them of their use case."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"/figma/../img/figma_error.png",alt:"Ad hoc question example"})}),"\n",(0,s.jsx)(n.p,{children:"Other ad hoc conversations, such as talking about the weather or saying hello help build a bond between user and chatbot. Although we do not recommend training your chatbot to tell jokes with users in industrial settings, a certain number of ad hoc conversations are beneficial."}),"\n",(0,s.jsx)(n.p,{children:"Finally, in the face of truly unprofessional and offensive input, the following example from Microsoft Copilot shows a clear set of standards and makes their ethical guidelines transparent:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Copilot:"})," As an AI language model, I don't have personal opinions or feelings, but I am designed to follow ethical guidelines. I ",(0,s.jsx)(n.strong,{children:"do not"})," engage in discussions or create content that promotes racism, sexism, or any form of discrimination. If you encounter any inappropriate responses, please let me know, and I'll do my best to address them."]}),"\n",(0,s.jsx)(n.h2,{id:"dos-and-donts",children:"Dos and Don\u2019ts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Do consider training your chatbot to filter for a list of offensive terms"}),"\n",(0,s.jsx)(n.li,{children:"Do consider localization and cultural nuances, such as \u201cmaster\u201d and \u201cslave\u201d"}),"\n",(0,s.jsx)(n.li,{children:"Do train your chatbot to move users back on track"}),"\n",(0,s.jsx)(n.li,{children:"Don\u2019t forget to test and train for offensive user input"}),"\n",(0,s.jsx)(n.li,{children:"Don\u2019t forget to allow for some harmless non-work interactions"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},16436:(e,n,o)=>{o.d(n,{Z:()=>r,a:()=>a});var s=o(52983);const i={},t=s.createContext(i);function a(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);